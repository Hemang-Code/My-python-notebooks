{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SRtRf8f_h4-"
   },
   "source": [
    "# Capstone Project Problem Statement: Forecasting Power Consumption in Tetuan City Using Environmental and Solar Radiation Indicators\n",
    "\n",
    "## Business Context:\n",
    "Energy consumption is a critical concern for modern organizations aiming to reduce operational costs, optimize energy usage, and support environmental sustainability. With advancements in sensor technologies and IoT, real-time data on environmental conditions (like temperature, humidity, wind speed, and solar radiation) can be collected alongside energy usage statistics. Leveraging this data enables organizations to uncover patterns, forecast energy demands, and implement smart energy-saving strategies across various zones of a facility or campus.\n",
    "\n",
    "This project analyzes multivariate environmental and energy data to uncover insights and build predictive models to aid in efficient energy management across three distinct zones. The ultimate goal is to enable data-driven decisions to reduce waste and improve energy efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pieBtCrT_h5A"
   },
   "source": [
    "# Project Description: Tetuan City Energy Consumption Analysis\n",
    "\n",
    "This dataset contains hourly records of environmental conditions and electricity usage from Tetuan City. It includes variables such as temperature, humidity, wind speed, solar radiation (general and diffuse flows), and power consumption across three distinct zones. Using this dataset, I aim to perform exploratory data analysis to uncover patterns and relationships between weather conditions and energy usage. The project also involves building machine learning models to solve both regression and classification problems—predicting actual power consumption values and classifying usage levels as high or low. This end-to-end workflow will serve as my capstone project, showcasing how data science can be applied to optimize energy efficiency and support smart city initiatives. If time permits, the project may also include deployment of the predictive model for real-time use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fxZPQGV_h5B"
   },
   "source": [
    "# Project Objective:\n",
    "\n",
    "The project aims to perform a comprehensive analysis of zone-wise power consumption using weather and radiation parameters. It includes multiple tasks from data understanding to building predictive models:\n",
    "\n",
    "a. Exploratory Data Analysis (EDA):\n",
    "* Understand the distribution and relationships of variables such as Temperature, Humidity, Wind Speed, and different types of solar radiation (general   diffuse flows and diffuse flows).\n",
    "* Visualize and analyze power consumption patterns in Zone 1, Zone 2, and Zone 3 over time.\n",
    "* Identify outliers, missing values, trends, seasonality, and correlations between environmental conditions and power consumption.\n",
    "\n",
    "b. Classification Task (Binary Classification):\n",
    "* Objective: Classify high vs. low power consumption for a selected zone (e.g., Zone 1).\n",
    "* Method:\n",
    "   * Define a binary target variable: e.g., Power consumption above a certain threshold = 1 (high), else = 0 (low).\n",
    "   * Use features like temperature, humidity, wind speed, and radiation data as inputs.\n",
    "   * Apply classification models such as Logistic Regression, Random Forest, or XGBoost.\n",
    "   * Evaluate class balance and apply resampling if necessary.\n",
    "\n",
    "c. Regression Task (Predictive Modeling):\n",
    "* Objective: Predict actual power consumption (e.g., Zone 1 Power Consumption) using the environmental variables.\n",
    "* Method:\n",
    "   * Use models such as Linear Regression, Decision Trees, Random Forest, or Gradient Boosting Regressors.\n",
    "   * Compare models using metrics like RMSE, MAE, and R².\n",
    "   * Perform feature importance analysis to determine key contributors to energy usage.\n",
    "\n",
    "d. Model Evaluation:\n",
    "* Evaluate both classification and regression models using appropriate metrics:\n",
    "  * Classification: Accuracy, Precision, Recall, F1-Score, ROC-AUC.\n",
    "  * Regression: R² Score, Mean Squared Error (MSE), Mean Absolute Error (MAE), RMSE.\n",
    "* Use cross-validation and hyperparameter tuning (Grid Search/CV) to ensure model robustness and generalizability.\n",
    "\n",
    "e. Model Deployment:\n",
    "* If time permits, deploy the best-performing model using:\n",
    "* A Flask or FastAPI web application.\n",
    "* Streamlit for interactive visualization and prediction interface.\n",
    "* Export model via joblib or pickle for real-time inference.\n",
    "* Demonstrate how to input new environmental data and predict power consumption or classification outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5E_Kj3BNWIWZ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeZ9e2Yo_h5B"
   },
   "source": [
    "## Data Description\n",
    "The dataset contains the following columns:\n",
    "\n",
    "* DateTime: Timestamp of the observation (hourly).\n",
    "* Temperature: Ambient temperature (°C).\n",
    "* Humidity: Humidity percentage (%).\n",
    "* Wind Speed: Wind speed at the time of observation (m/s).\n",
    "* General Diffuse Flows: Total diffuse solar radiation measured.\n",
    "* Diffuse Flows: Diffuse component of solar radiation.\n",
    "* Zone 1 Power Consumption: Power usage in Zone 1 (kW).\n",
    "* Zone 2 Power Consumption: Power usage in Zone 2 (kW).\n",
    "* Zone 3 Power Consumption: Power usage in Zone 3 (kW)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoYJfubtOxns"
   },
   "source": [
    "# 1.Exploratory *Data* Analysis:\n",
    "\n",
    "# EDA ALL STEPS:\n",
    "* Load Data\t                     `✅`\n",
    "* Check structure\t               ✅\n",
    "* Summary stats                  ✅\n",
    "* Missing values\t               ✅\n",
    "* Duplicates\t                   ✅\n",
    "* Univariate analysis            ✅\n",
    "* Bivariate analysis\t           ✅\n",
    "* Correlation\t                   ✅\n",
    "* Outliers\t                     ✅\n",
    "* Feature engineering (optional) ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spVqt7fA_vGP"
   },
   "source": [
    "# Importing Libreries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Srz7nDaupJaB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mlt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OxaTfYI0nz1f"
   },
   "source": [
    "#### Observation:\n",
    "##### This cell imports necessary Python libraries for data manipulation (numpy, pandas) and data visualization (matplotlib, seaborn). These are      essential for handling datasets and plotting graphs during exploratory data analysis (EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytv_Fgrznz1g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBTakTOV_9XN"
   },
   "source": [
    "# Reading the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GmzrarkmOx2E"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Tetuan City power consumption.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71LIuaQnnz1g"
   },
   "source": [
    "### Observation:\n",
    "##### This line reads the CSV file named \"Tetuan City power consumption.csv\" into a pandas DataFrame named df. This is the primary dataset that will be used for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gBM5nFNnz1g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgOHPp2pABqy"
   },
   "source": [
    "# Displaying Initial Records of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "5MxqEOQyPjPQ",
    "outputId": "f1843925-bb49-4dd4-cb7a-07017bac6538"
   },
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqjIwlJ9nz1h"
   },
   "source": [
    "#### Observation:\n",
    "##### This cell displays the first 3 rows of the DataFrame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rL_Gyavtnz1h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0m4oTUJEnz1h"
   },
   "source": [
    "# Displaying Last 5 Records of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 348
    },
    "id": "gPX2Rt7hhnrT",
    "outputId": "95e2b670-a5b4-4ee0-b531-c49890697b12"
   },
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apbilhCDnz1h"
   },
   "source": [
    "#### Observation:\n",
    "##### This cell displays the last 5 rows of the DataFrame df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3VtpdvCnz1h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1WEWKc5nz1h"
   },
   "source": [
    "# Checking the Shape of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9s5A7Q4VPtkz",
    "outputId": "91ecf4be-b58e-43ab-f20c-0278cc978f73"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iI_91nsInz1h"
   },
   "source": [
    "#### Observation:\n",
    "##### This cell returns the dimensions of the DataFrame df in the form (rows, columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VbfVa9Pnz1h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yz1zkgcYnz1h"
   },
   "source": [
    "# Listing All Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Via4lfuMQD2c",
    "outputId": "5cfff07e-46a7-4406-91ce-5a31927e01f9"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPsBz6abnz1i"
   },
   "source": [
    "#### Observation:\n",
    "##### This cell displays the names of all columns in the DataFrame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8YnTYFvnz1i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtS-D3cJnz1i"
   },
   "source": [
    "# Displaying Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aDJxacNpRh7U",
    "outputId": "56a2472f-7bb1-46f8-e966-f9f977d331a3"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rHT8O8onz1i"
   },
   "source": [
    "#### Observation:\n",
    "* This cell provides a summary of the DataFrame’s structure, including:\n",
    "* The number of entries (rows).\n",
    "* Each column name and its data type.\n",
    "* The count of non-null (non-missing) values in each column.\n",
    "* The memory usage of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "up7ANskQnz1i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xl_oQYuDnz1i"
   },
   "source": [
    "# Generating Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "id": "9yLKYQ_8Rm0J",
    "outputId": "b6d6716e-2929-4270-9e8c-5426a6ea4337"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsnAnlr0nz1i"
   },
   "source": [
    "#### Observation:\n",
    "* This cell generates summary statistics for all numeric columns in the dataset. It includes:\n",
    "* Count: Number of non-null entries.\n",
    "* Mean: Average value.\n",
    "* Standard deviation (std): Spread of data.\n",
    "* Min and Max: Minimum and maximum values.\n",
    "* 25%, 50%, 75%: Quartile values (useful for detecting skewness or outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAxs2mEMnz1i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LIMnoT3Sigx"
   },
   "source": [
    "# Check for Missing Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "plC6IUgyhrtw",
    "outputId": "2ed02480-7d10-4db1-b747-4d247a259d9d"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofP8YtPhnz1j"
   },
   "source": [
    "#### Observation:\n",
    "* This cell checks for missing (null/NaN) values in each column of the DataFrame by:\n",
    "* Using isnull() to create a boolean mask where True indicates missing values.\n",
    "* Using sum() to count the total True values in each column.\n",
    "* This is a crucial step in data cleaning, helping to:\n",
    "  * Detect columns that require imputation or removal.\n",
    "  * Ensure the dataset is complete before modeling or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sw5ZY2Uonz1j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5zT_EUynz1j"
   },
   "source": [
    "# Checking for Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XivY4R7znz1j",
    "outputId": "fe02338c-89bb-4abc-d945-e23d31571654"
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve1gs4brnz1j"
   },
   "source": [
    "#### Observation:\n",
    "* This cell checks for duplicate records in the dataset:\n",
    "  * df.duplicated() returns a Boolean Series indicating which rows are duplicates.\n",
    "  * sum() counts the number of True values, i.e., the total number of duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3g6smuinz1j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HK6jE_eWnz1j"
   },
   "source": [
    "# Visualizing Missing Data Using Missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "Ob7GDRsmh1L7",
    "outputId": "c5a83a56-b2e7-46af-d5f1-0241e175237c"
   },
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "msno.matrix(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btCeXBp0nz1k"
   },
   "source": [
    "#### Observation:\n",
    "* This cell uses the missingno library to visually inspect missing data in the DataFrame using a matrix plot:\n",
    "  * Each column is shown as a vertical bar.\n",
    "  * White lines (if any) indicate missing entries.\n",
    "  * The plot also shows a sparkline indicating data density across rows.\n",
    "\n",
    "* This graphical tool helps to:\n",
    "  * Quickly identify patterns or blocks of missing data.\n",
    "  * Detect if missing values are randomly distributed or occur in chunks.\n",
    "  * Complement the numeric check from df.isnull().sum()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xjuug3lnz1k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRIGcyb4yV_l"
   },
   "source": [
    "# Univariate Analysis:–\n",
    "### Histogram and KDE Plot for Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "4UJ6mTCgmFfb",
    "outputId": "1f6da9d4-76ff-4dce-9549-c20ccdab4c4e"
   },
   "outputs": [],
   "source": [
    "sns.histplot(df['Temperature'],kde = True)\n",
    "plt.title(\"Temp distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ER58wnArnz1k"
   },
   "source": [
    "#### Observation:\n",
    "* This is a univariate analysis of the Temperature column using a histogram with a Kernel Density Estimate (KDE) overlay:\n",
    "* The histogram shows the frequency distribution of temperature values.\n",
    "* The KDE curve estimates the probability density, giving a smooth distribution shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hNT04X-nz1k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlQNyMkJnz1k"
   },
   "source": [
    "# Univariate Analysis:–\n",
    "### Average Power Consumption by Zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "id": "TmADnCVWpaxH",
    "outputId": "086334d3-e00c-40f2-8be6-462998b6fe32"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# Normalize column names: remove extra spaces\n",
    "df.columns = [re.sub(' +', ' ', col.strip()) for col in df.columns]\n",
    "\n",
    "# Plot average power consumption\n",
    "df[['Zone 1 Power Consumption', 'Zone 2 Power Consumption', 'Zone 3 Power Consumption']].mean().plot(kind='bar')\n",
    "plt.title(\"Average Power Consumption by Zone\")\n",
    "plt.ylabel(\"Power Consumption\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPh12VK5nz1k"
   },
   "source": [
    "#### Observation:\n",
    "* This cell performs column name cleaning using regex to remove extra spaces, ensuring reliable column access.\n",
    "* It then performs univariate analysis by plotting the average power consumption for each zone (Zone 1, Zone 2, and Zone 3) using a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dhpwzvinz1k"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARgdeDL0yNj8"
   },
   "source": [
    "# Bivariate Analysis:–\n",
    "### Humidity Bins vs Zone 1 Power Consumption (Boxplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "esAHYVybxKzo",
    "outputId": "9691d340-c740-4cde-a134-63b05f8adda6"
   },
   "outputs": [],
   "source": [
    "df['Humidity Bin'] = pd.cut(df['Humidity'], bins=5)\n",
    "sns.boxplot(x='Humidity Bin', y='Zone 1 Power Consumption', data=df)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Humidity Bins vs. Zone 1 Power Consumption')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWZDW5vLnz1k"
   },
   "source": [
    "#### Observation:\n",
    "* This cell performs a bivariate analysis between Humidity (binned) and Zone 1 Power Consumption using a boxplot.\n",
    "* pd.cut() divides the humidity values into 5 equal-width bins to simplify analysis.\n",
    "* The boxplot shows the distribution (median, quartiles, and outliers) of power consumption within each humidity range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJABpAggnz1l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imoiIHVCnz1l"
   },
   "source": [
    "# Bivariate Analysis:–\n",
    "### Temperature vs Zone 1 Power Consumption (Scatter Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "UgM1HMbDxXn9",
    "outputId": "f101b371-646f-4461-d9cd-15ea84df1460"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Temperature', y='Zone 1 Power Consumption', data=df)\n",
    "plt.title('Temperature vs. Zone 1 Power Consumption')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CnsqUiinz1l"
   },
   "source": [
    "#### Observation:\n",
    "* This scatter plot performs a bivariate analysis between Temperature and Zone 1 Power Consumption.\n",
    "* Each point represents a record in the dataset with:\n",
    "  * X-axis: Temperature value\n",
    "  * Y-axis: Corresponding Zone 1 power usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rg0vm8ASnz1l"
   },
   "source": [
    "# Bivariate Analysis:–\n",
    "### Wind Speed vs. High/Low Zone 1 Power Consumption (Stacked Bar Chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "WjkPzRJ3xdFD",
    "outputId": "3fb04194-8ff6-4974-b3ef-bfc0e2840111"
   },
   "outputs": [],
   "source": [
    "df['Wind Speed Bin'] = pd.cut(df['Wind Speed'], bins=4)\n",
    "df['High Power'] = df['Zone 1 Power Consumption'] > df['Zone 1 Power Consumption'].median()\n",
    "pd.crosstab(df['Wind Speed Bin'], df['High Power']).plot(kind='bar', stacked=True)\n",
    "plt.title('Wind Speed vs. High/Low Power Consumption')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctGVfBpGnz1l"
   },
   "source": [
    "#### Observation:\n",
    "* This cell analyzes how Zone 1 power consumption varies with wind speed, using a stacked bar chart.\n",
    "* Wind speed is binned into 4 intervals.\n",
    "* Power consumption is classified as High or Low based on whether it is above the median.\n",
    "* pd.crosstab() creates a contingency table, and the chart visualizes the count of high vs low consumption in each wind speed bin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzL0weiJnz1l"
   },
   "source": [
    "# Multivariate Analysis:–\n",
    "### Correlation Matrix of Numerical Features (Heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "id": "gpWI3TKmxmij",
    "outputId": "5eb08d7f-1162-4437-a00c-7a9535dac6d8"
   },
   "outputs": [],
   "source": [
    "corr = df.corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-Fe7Jn8yhfD"
   },
   "source": [
    "# Observation:\n",
    "* This heatmap visualizes the pairwise correlation between all numeric columns in the dataset.\n",
    "* Correlation values range from -1 to 1:\n",
    "   * +1: Perfect positive correlation\n",
    "   * -1: Perfect negative correlation\n",
    "   * 0: No correlation\n",
    "* annot=True displays the exact correlation values inside the heatmap cells.\n",
    "* coolwarm color palette helps easily distinguish positive (warm) and negative (cool) relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItDv1NzoTBKu"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHFRlMqZyoY5"
   },
   "source": [
    "# Outlier Detection Using Boxplots and the IQR Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Hgs9sR_nyuV3",
    "outputId": "5e1eba4f-c95a-4143-e2ea-2e5c5db517bf"
   },
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include='number').columns\n",
    "\n",
    "# Boxplots for each numeric column to visually inspect outliers\n",
    "for col in numeric_cols:\n",
    "    plt.figure(figsize=(6, 1.5))\n",
    "    sns.boxplot(x=df[col])\n",
    "    plt.title(f'Boxplot for {col}')\n",
    "    plt.show()\n",
    "\n",
    "# Detect outliers using the IQR method\n",
    "outlier_summary = {}\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    outlier_summary[col] = len(outliers)\n",
    "\n",
    "# Print number of outliers in each column\n",
    "for col, count in outlier_summary.items():\n",
    "    print(f\"{col}: {count} outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn2yeDcUnz1m"
   },
   "source": [
    "#### Observation:\n",
    "* Numeric Column Selection:\n",
    "  * Selected only the numeric columns from the DataFrame for outlier analysis.\n",
    "* Boxplot Visualization:\n",
    "  * Generated boxplots for each numeric column to visually inspect the spread and detect potential outliers.\n",
    "* IQR Calculation:\n",
    "  * Computed the Interquartile Range (IQR) for each column using Q1 (25th percentile) and Q3 (75th percentile).\n",
    "* Outlier Detection Logic:\n",
    "  * Defined outliers as data points below Q1 - 1.5 × IQR or above Q3 + 1.5 × IQR.\n",
    "* Outlier Counting:\n",
    "  * Counted and stored the number of outliers for each numeric column in a summary dictionary.\n",
    "* Output Summary:\n",
    "  * Printed the number of detected outliers for every numeric column, helping to identify which features contain extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wAOHNav6DGJ"
   },
   "source": [
    "# 3- Data Cleaning & pre processing\n",
    "* 0.Dropping unwanted columns\n",
    "* 1.Dropping duplicate rows\n",
    "* 2.Replacing wrong entries\n",
    "* 3.Missing values imputation (SimpleImputer, fillna())\n",
    "* 4.Handle outliers (IQR, Z-score method)\n",
    "* 5.Encoding\n",
    "* 6.Data splitting\n",
    "* 7.Feature scaling: StandardScaler, MinMaxScaler\n",
    "* 8.Feature selection:Based on correlation, domain knowledge, or model-based methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHLpVjv06TDv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('/content/Tetuan City power consumption.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BrvXIVpd6UYj",
    "outputId": "65c066e6-8bd6-4bf9-d858-26b8717006eb"
   },
   "outputs": [],
   "source": [
    "# Make a copy of the original dataset\n",
    "df_copy = df.copy()\n",
    "# Select categorical variables(Object type)\n",
    "cat_variables = df_copy.select_dtypes(include = 'object')\n",
    "# Select numerical varialbles(int,float)\n",
    "num_variables = df_copy.select_dtypes(include = ['int','float'])\n",
    "\n",
    "# Print Results\n",
    "print(\"Categorical Varibles:\")\n",
    "print(cat_variables.head())\n",
    "print(\"\\nNumerical variables:\")\n",
    "print(num_variables.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LOMTJLxDikE"
   },
   "source": [
    "### 3.0 Dropping unwanted columns:\n",
    "* Suppose we identify some unwanted columns (example: 'Unnamed: 0' or any irrelevant column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sKfcFigfDr__",
    "outputId": "1bbbf301-809d-4e55-c791-9af324658f0d"
   },
   "outputs": [],
   "source": [
    "# print columns of dataset\n",
    "print('columns in data set')\n",
    "print(df_copy.columns)\n",
    "\n",
    "# If you don't want to drop anything, keep the list empty(as it is)\n",
    "unwanted_columns = []\n",
    "\n",
    "# Check if the unwanted columns actually exist in the dataframe\n",
    "columns_to_drop = [col for col in unwanted_columns if col in df_copy.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFX_t4bM6_HK"
   },
   "source": [
    "### 3.1 Dropping duplicates Rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "8V0M_VNo7Gkw",
    "outputId": "60efd542-bfb9-47ea-a433-939c852a3148"
   },
   "outputs": [],
   "source": [
    "# Find all duplicate rows\n",
    "duplicates = df_copy[df.duplicated()]\n",
    "\n",
    "#show number of duplicated rows\n",
    "print(\"number of dupliate rows:\",duplicates.shape[0])\n",
    "\n",
    "# preview duplicates, if any\n",
    "duplicates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkOd7S7H8Xjq",
    "outputId": "9e6f233f-ebe4-42d9-e492-5232ffd9c9e6"
   },
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# check new shape\n",
    "print(\"New shape after removing:\",df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6A3kLnMi-w-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJqC4UiT8-b9"
   },
   "source": [
    "### 3.2 Replacing wrong entries, if any  \n",
    "\n",
    "* Missing or NaN\n",
    "\n",
    "* Outliers or unrealistic values\n",
    "\n",
    "* Wrong data types\n",
    "\n",
    "* Duplicate timestamps or logic issues\n",
    "\n",
    "* Inconsistent column formats or typos\n",
    "\n",
    "## steps:\n",
    "* 1.load data set(already done in EDA)\n",
    "* 2.detect wrong entries\n",
    "* 3.relace or fixed wrong entries\n",
    "* 4.save the clean file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yRxuIBsZ_tU"
   },
   "source": [
    "#####  3.2.1Load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Po0HGO9LZ3nW"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Load the CSV file\n",
    "# df = pd.read_csv(\"etuan City power consumption.csv\")\n",
    "\n",
    "# Show first 5 rows\n",
    "# df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtIeVvj-aI4k"
   },
   "source": [
    "##### 3.2.2 detect wrong entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "BZg94Lz1Z3aF",
    "outputId": "5a00d1e0-1c33-4edc-f941-acc1d3f216ab"
   },
   "outputs": [],
   "source": [
    "# Check for Null/Missing Values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "qN-gUNsnZ3UV",
    "outputId": "4faea1ae-5d70-4899-f8b0-aca613e4fef5"
   },
   "outputs": [],
   "source": [
    "# Check Data Types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcXY1-MGb91O"
   },
   "source": [
    "#### Detect outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_XWRGK6vb0aq",
    "outputId": "067db58a-2e88-4057-9999-955af6a0ad7a"
   },
   "outputs": [],
   "source": [
    "# Detect outliers:\n",
    "\n",
    "# Describe stats to spot anomalies\n",
    "# df.describe()\n",
    "\n",
    "# Check negative values in power consumption (if not expected)\n",
    "invalid_power = df[(df[\"Zone 1 Power Consumption\"] < 0) |\n",
    "                   (df[\"Zone 2  Power Consumption\"] < 0) |\n",
    "                   (df[\"Zone 3  Power Consumption\"] < 0)]\n",
    "print(invalid_power)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obToFGHtcbpA"
   },
   "source": [
    "##### 3.2.3 Replace or fix wrong entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9nec2EqcWcL"
   },
   "outputs": [],
   "source": [
    "# Replace Negative Values with Mean of the Column\n",
    "for col in [\"Zone 1 Power Consumption\", \"Zone 2  Power Consumption\", \"Zone 3  Power Consumption\"]:\n",
    "    mean_val = df[df[col] >= 0][col].mean()\n",
    "    df[col] = df[col].apply(lambda x: mean_val if x < 0 else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2MNl5qccrYC",
    "outputId": "a0a90ce0-a769-4fb8-84cb-1c3789d27080"
   },
   "outputs": [],
   "source": [
    "# Fill Missing Values with Forward Fill\n",
    "df.fillna(method='ffill', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MWLeeGGc186"
   },
   "source": [
    "##### Replace Strings or Wrong Entries\n",
    " * (Example: replace misspelled entries in a \"City\" column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44IDQM8qcrOY"
   },
   "outputs": [],
   "source": [
    "# df[\"City\"] = df[\"City\"].replace({\"Tetuon\": \"Tetuan\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khCTWiVNcrHX"
   },
   "outputs": [],
   "source": [
    "# Save the Cleaned File\n",
    "df.to_csv(\"Cleaned_Tetuan_City_power.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91w9DFTQePt0"
   },
   "source": [
    "### 3.3 Missing values imputation (SimpleImputer, fillna())\n",
    "Missing data is common in real-world datasets. If not handled, it can lead to:\n",
    "\n",
    "Errors in model training\n",
    "\n",
    "Biased or incomplete analysis\n",
    "\n",
    "Failures in ML algorithms (most don’t accept nulls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUjs1TqbiE5z"
   },
   "outputs": [],
   "source": [
    "# Make a copy before imputing\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDxlkoTfeOXl"
   },
   "outputs": [],
   "source": [
    "# Perform imputation...\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_cols = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Create imputer: strategy = mean, median, most_frequent, or constant\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Fit and transform\n",
    "df[numeric_cols.columns] = imputer.fit_transform(numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FUDnCXeleOVQ",
    "outputId": "5198496b-17d4-4cb2-dfca-385f131aa75a"
   },
   "outputs": [],
   "source": [
    "#Then compare\n",
    "print(\"Before:\\n\", df_original[numeric_cols.columns].isnull().sum())\n",
    "print(\"After:\\n\", df[numeric_cols.columns].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mJZOe85WeOS6",
    "outputId": "4b13785f-213c-4d1b-ac4b-f2042a2fb55f"
   },
   "outputs": [],
   "source": [
    "# If you want to check if any missing values remain:\n",
    "print(df.isnull().sum())        # Shows number of missing values in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhsSSz7GjA0c"
   },
   "source": [
    "### 3.4 Handle outliers (IQR method) :\n",
    "* IQR = Q3 − Q1\n",
    "\n",
    "* Q1 = 25th percentile\n",
    "* Q3 = 75th percentile\n",
    "\n",
    "* Lower bound = Q1 − 1.5 × IQR\n",
    "\n",
    "* Upper bound = Q3 + 1.5 × IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iDAQD2DVeOQ1",
    "outputId": "997489dc-a577-4704-afd5-f50b194009b5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example for one column\n",
    "col = 'Temperature'  # replace with your column name\n",
    "\n",
    "Q1 = df[col].quantile(0.25)\n",
    "Q3 = df[col].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out outliers\n",
    "outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "print(\"Outliers:\\n\", outliers)\n",
    "\n",
    "# Optionally remove them:\n",
    "df_cleaned = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic54mfWUjm6f"
   },
   "source": [
    "#### Z-score Method\n",
    "* Z-score = (x − mean) / std\n",
    "\n",
    "* Outliers have Z-scores > 3 or < -3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKaMp-4peOOe",
    "outputId": "12d945c7-0251-4989-cc9f-783feb63946b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"Tetuan City power consumption.csv\")\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "# Compute Z-scores\n",
    "z_scores = zscore(numeric_df)\n",
    "\n",
    "# Convert back to DataFrame with same column names\n",
    "z_df = pd.DataFrame(z_scores, columns=numeric_df.columns)\n",
    "\n",
    "# Show Z-score output\n",
    "print(z_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eea6Wn3eoj0F"
   },
   "source": [
    "### 3.5 Encoding:\n",
    "* Load the dataset\n",
    "* Identify Categorical Columns\n",
    "* Option A: Label Encoding (for Ordinal/Ordered data)\n",
    "* Option B: One-Hot Encoding (for Nominal/Unordered data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ZfZj2EZeOL3",
    "outputId": "c824bcd9-2cb0-41a2-9b12-90106aa6bd0d"
   },
   "outputs": [],
   "source": [
    "# Show non-numeric (categorical) columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "print(\"Categorical Columns:\", categorical_cols.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaX-o41HpFI5"
   },
   "source": [
    "##### Label Encoding (for Ordinal/Ordered data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fDZ1mhyJeOJS",
    "outputId": "bbc3dccb-4d72-4484-9921-310942ad4c56"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to all categorical columns\n",
    "for col in categorical_cols:\n",
    "    df[col + '_label'] = le.fit_transform(df[col])\n",
    "\n",
    "# Show result\n",
    "df[[*categorical_cols, *[col + '_label' for col in categorical_cols]]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WH9xvigpK1s"
   },
   "source": [
    "#####  One-Hot Encoding (for Nominal/Unordered data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "EKU8zMFJo__-",
    "outputId": "ff77bb15-3da6-4083-9235-65a5c8a5eabe"
   },
   "outputs": [],
   "source": [
    "# One-hot encode categorical columns (remove first column to avoid dummy variable trap)\n",
    "df_onehot = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Show result\n",
    "df_onehot.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zO9aT0te_sJ5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfXvRupI_sDb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fb7TNP0QxUVc"
   },
   "source": [
    "#### 3.6 Data Splitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znF4p-0cyLgI",
    "outputId": "ac238fa9-04c6-4a8a-9566-3b0f27a02efa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load your dataset\n",
    "df = pd.read_csv(\"Tetuan City power consumption.csv\")\n",
    "# We are predicting \"Zone 1 Power Consumption\"\n",
    "target_column = \"Zone 1 Power Consumption\"\n",
    "# X = all columns except the target\n",
    "X = df.drop(columns=[target_column])\n",
    "\n",
    "# y = only the target column\n",
    "y = df[target_column]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,         # input features\n",
    "    y,         # target values\n",
    "    test_size=0.2,      # 20% for test, 80% for training\n",
    "    random_state=42     # ensures same split every time (reproducibility)\n",
    ")\n",
    "\n",
    "# Multiple outputs neatly\n",
    "{\n",
    "    \"X_train\": X_train.shape,\n",
    "    \"X_test\": X_test.shape,\n",
    "    \"y_train\": y_train.shape,\n",
    "    \"y_test\": y_test.shape\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vba9GoXYyLdj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_D8golB8yY3J"
   },
   "source": [
    "#### 3.7 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbfNWLNtyLXT",
    "outputId": "8a164bbb-5b2b-428b-c255-d0fcd523c474"
   },
   "outputs": [],
   "source": [
    "# If X_train is a DataFrame (not NumPy array), drop the DateTime column\n",
    "X_train = X_train.drop(columns=['DateTime'], errors='ignore')\n",
    "X_test = X_test.drop(columns=['DateTime'], errors='ignore')\n",
    "\n",
    "# Now apply scaling\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# StandardScaler\n",
    "standard_scaler = StandardScaler()\n",
    "X_train_standard = standard_scaler.fit_transform(X_train)\n",
    "X_test_standard = standard_scaler.transform(X_test)\n",
    "\n",
    "# MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "X_train_minmax = minmax_scaler.fit_transform(X_train)\n",
    "X_test_minmax = minmax_scaler.transform(X_test)\n",
    "\n",
    "print(\"✅ Scaling done!\")\n",
    "print(\"Standard Scaled X_train shape:\", X_train_standard.shape)\n",
    "print(\"MinMax Scaled X_train shape:\", X_train_minmax.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neZphan3y4LU"
   },
   "source": [
    "#### 3.8 Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjlCtrJ7y9_N"
   },
   "source": [
    "##### 3.8.1  Based on Correlation (Filter method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "id": "uK0e7H2Jyoj0",
    "outputId": "4a96b316-728f-4f2f-d467-532bf5501f68"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = X_train.corr()\n",
    "\n",
    "# Plot heatmap (optional)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Drop features highly correlated with others (e.g., corr > 0.9)\n",
    "threshold = 0.9\n",
    "upper = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "to_drop = [column for column in upper.columns if any(upper[column].abs() > threshold)]\n",
    "\n",
    "# Drop from dataset\n",
    "X_train_corr = X_train.drop(columns=to_drop)\n",
    "X_test_corr = X_test.drop(columns=to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B2uzNGaazPSw"
   },
   "source": [
    "##### 3.8.2  Model-Based Selection (Wrapper or Embedded methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "nu96Sc1byzPb",
    "outputId": "98535871-74f9-49b8-93bb-42aaf7e56457"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "\n",
    "# Fit a model\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "importances.sort_values(ascending=False).plot(kind='barh')\n",
    "plt.title(\"Feature Importance (Random Forest)\")\n",
    "plt.show()\n",
    "\n",
    "# Select top N features\n",
    "top_features = importances.sort_values(ascending=False).head(5).index.tolist()\n",
    "X_train_model = X_train[top_features]\n",
    "X_test_model = X_test[top_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzSqhfqkyoqF"
   },
   "source": [
    "# 4-Model Building (Regression), Evaluation & Tuning\n",
    "\n",
    "### 4.1 Regression algorithms\n",
    "    * Linear Regression\n",
    "    * KNN\n",
    "    * Decision Trees (CART)\n",
    "    * Random Forest\n",
    "    * Boosting\n",
    "        * Adaboost,\n",
    "        * Gboost,\n",
    "        * XGboost\n",
    "### 4.2 Model Evaluation: Regression metrics: R² & RMSE\n",
    "1. R-squared (R²) — Coefficient of Determination\n",
    "    * What it means:\n",
    "        * Measures how well the model explains the variability in the target variable.\n",
    "        * Value lies between 0 and 1 (can be negative if model performs worse than the mean).\n",
    "    * Interpretation:\n",
    "        * R² = 1 → perfect prediction\n",
    "        * R² = 0 → model is no better than the average\n",
    "        * Higher is better\n",
    "          ![image.png](attachment:e23679fd-3fe1-4ef2-b9e9-1dfed173e585.png)\n",
    "\n",
    "2. RMSE — Root Mean Squared Error\n",
    "    * What it means:\n",
    "        * Measures average prediction error in the same units as the target variable.\n",
    "        * It gives more weight to larger errors.\n",
    "    * Interpretation:\n",
    "        * Lower is better\n",
    "        * Easy to interpret because it’s in the same unit as\n",
    "          ![image.png](attachment:65110f95-03fd-4549-be04-631970e6b345.png)\n",
    "### 4.3 Model Tuning\n",
    "    * GridSearchCV\n",
    "    * Hyper Parameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "03lPoSzuuUbW",
    "outputId": "1bc7f3eb-35bb-40f1-c6da-34f694e2f41d"
   },
   "outputs": [],
   "source": [
    "# 📌 Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 📌 Load the dataset\n",
    "df = pd.read_csv(\"Tetuan City power consumption.csv\")\n",
    "\n",
    "# 📌 Select features and target\n",
    "features = ['Temperature', 'Humidity', 'Wind Speed']\n",
    "target = 'Zone 1 Power Consumption'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# 📌 Train-Test Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 📌 Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# 📌 Define models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"KNN\": KNeighborsRegressor(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"AdaBoost\": AdaBoostRegressor(),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
    "    \"XGBoost\": XGBRegressor(objective='reg:squarederror')\n",
    "}\n",
    "\n",
    "# 📌 Train and evaluate models\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    y_val_pred = model.predict(X_val_scaled)\n",
    "\n",
    "    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    rmse_val = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    r2_val = r2_score(y_val, y_val_pred)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Train RMSE\": rmse_train,\n",
    "        \"Train R²\": r2_train,\n",
    "        \"Val RMSE\": rmse_val,\n",
    "        \"Val R²\": r2_val\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df.sort_values(\"Val RMSE\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2LnCCgYuUW7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsP9oY0suUUm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uQvKo08uUOn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
